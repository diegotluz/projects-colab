{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diegotluz/projects-colab/blob/main/Aula_2_6_DTS_PLN_Exerc%C3%ADcios.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FdxZAH6TWPH"
      },
      "source": [
        "#**Exercícios - Aula 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dado o dataset de produtos [1], desenvolva os seguintes pipelines:\n",
        "\n",
        "[1] - https://dados-ml-pln.s3-sa-east-1.amazonaws.com/produtos.csv\n",
        "\n",
        "Obs.: em todos os pipelines use:\n",
        "- normalização renovendo valores faltantes\n",
        "- criem uma nova coluna concatenando as colunas nome e descrição.\n",
        "- randon_state igual a 42 para permitir a comparação com seus colegas e separe uma amostra de 30% para teste.\n"
      ],
      "metadata": {
        "id": "b_lUc0-cLsVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"https://dados-ml-pln.s3-sa-east-1.amazonaws.com/produtos.csv\", delimiter=\";\", encoding='utf-8')"
      ],
      "metadata": {
        "id": "UUDRZ7FrI4fk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. ) Treine um modelo de classificação DecisionTreeClassifier do pacote scikit-learn para classificar os produtos em suas categorias, com as seguintes configurações usando a nova coluna (nome + descricao):**\n",
        "\n",
        "- 1.1 Contagem de termos simples com unigrama e sem stop-words.\n",
        "- 1.2 Contagem de termos simples com unigrama + brigrama e sem stop-words.\n",
        "- 1.3 TF-IDF com unigrama e com stop-words.\n",
        "- 1.4 TF-IDF com unigrama e sem stop-words.\n",
        "- 1.5 TF-IDF com unigrama e sem stop-words em textos lematizados (Dica: crie uma função para lematizar o texto usando o Spacy)."
      ],
      "metadata": {
        "id": "hEgJjADlSCWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn pandas nltk spacy >> /dev/null\n",
        "!python -m spacy download pt_core_news_sm --quiet\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0utSwYcK2O2",
        "outputId": "dabae5d2-5a1d-45c2-d5fb-53e21d506289"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import spacy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Carregue o dataset\n",
        "df = pd.read_csv(\"https://dados-ml-pln.s3-sa-east-1.amazonaws.com/produtos.csv\", delimiter=\";\", encoding='utf-8')\n",
        "\n",
        "# Limpeza e pré-processamento\n",
        "df.dropna(inplace=True)\n",
        "df[\"texto\"] = df['nome'] + \" \" + df['descricao']\n",
        "\n",
        "# Função para lematizar o texto\n",
        "nlp = spacy.load(\"pt_core_news_sm\")\n",
        "def lematizar_texto(texto):\n",
        "    doc = nlp(texto)\n",
        "    tokens_lematizados = [token.lemma_ for token in doc if not token.is_stop]\n",
        "    return \" \".join(tokens_lematizados)\n",
        "\n",
        "df['texto_lematizado'] = df['texto'].apply(lematizar_texto)\n",
        "\n",
        "# Divisão da amostra entre treino e teste\n",
        "df_train, df_test = train_test_split(df, test_size=0.3, random_state=42)\n",
        "\n",
        "# Carregue as stop words do NLTK\n",
        "stop_words = nltk.corpus.stopwords.words('portuguese')\n",
        "\n",
        "# Configurações dos modelos\n",
        "configs = [\n",
        "    {'vectorizer': CountVectorizer(stop_words=stop_words, ngram_range=(1, 1)), 'text_col': 'texto'},  # 1.1\n",
        "    {'vectorizer': CountVectorizer(stop_words=stop_words, ngram_range=(1, 2)), 'text_col': 'texto'},  # 1.2\n",
        "    {'vectorizer': TfidfVectorizer(ngram_range=(1, 1)), 'text_col': 'texto'},  # 1.3\n",
        "    {'vectorizer': TfidfVectorizer(stop_words=stop_words, ngram_range=(1, 1)), 'text_col': 'texto'},  # 1.4\n",
        "    {'vectorizer': TfidfVectorizer(ngram_range=(1, 1)), 'text_col': 'texto_lematizado'}  # 1.5\n",
        "]\n",
        "\n",
        "\n",
        "# Treinamento e avaliação dos modelos\n",
        "for i, config in enumerate(configs):\n",
        "    print(f\"Treinando modelo {i + 1}...\")\n",
        "\n",
        "    vectorizer = config['vectorizer']\n",
        "    text_col = config['text_col']\n",
        "\n",
        "    # Vetorização do dataframe de treino\n",
        "    vectorizer.fit(df_train[text_col])\n",
        "    x_train = vectorizer.transform(df_train[text_col])\n",
        "    y_train = df_train[\"categoria\"]\n",
        "\n",
        "    # Treinamento do modelo DecisionTreeClassifier\n",
        "    model = DecisionTreeClassifier(random_state=42)\n",
        "    model.fit(x_train, y_train)\n",
        "\n",
        "    # Transforma o dataframe de teste em vetor (features)\n",
        "    x_test = vectorizer.transform(df_test[text_col])\n",
        "\n",
        "    # Escoragem da classificação na amostra de teste\n",
        "    y_prediction = model.predict(x_test)\n",
        "\n",
        "    # Mensuração do resultado pela acurácia\n",
        "    y_test = df_test[\"categoria\"]\n",
        "    accuracy = accuracy_score(y_prediction, y_test)\n",
        "    print(f\"Acurácia do modelo {i + 1}: {accuracy}\")\n",
        "    print(\"-\" * 30)  # Separador visual entre os resultados"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12WGrrAvKufO",
        "outputId": "6ca57950-8bc6-47c5-b626-ae1a38405c01"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treinando modelo 1...\n",
            "Acurácia do modelo 1: 0.9611428571428572\n",
            "------------------------------\n",
            "Treinando modelo 2...\n",
            "Acurácia do modelo 2: 0.9565714285714285\n",
            "------------------------------\n",
            "Treinando modelo 3...\n",
            "Acurácia do modelo 3: 0.9531428571428572\n",
            "------------------------------\n",
            "Treinando modelo 4...\n",
            "Acurácia do modelo 4: 0.9565714285714285\n",
            "------------------------------\n",
            "Treinando modelo 5...\n",
            "Acurácia do modelo 5: 0.9314285714285714\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['texto'].lemma_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "gYT3c4fkL2B7",
        "outputId": "9a5228b6-2dc6-4ad5-81bd-47d8de067115"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Series' object has no attribute 'lemma_'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-60f122e5a33b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'texto'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6297\u001b[0m         ):\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6301\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'lemma_'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. ) Treine um modelo de classificação LogisticRegression do pacote scikit-learn para classificar os produtos em suas categorias, com as seguintes configurações:**\n",
        "\n",
        "- 2.1 Contagem de termos simples com unigrama e com stop-words.\n",
        "- 2.2 Contagem de termos simples com unigrama + brigrama e sem stop-words.\n",
        "- 2.3 TF-IDF com unigrama e sem stop-words.\n",
        "- 2.4 TF-IDF com unigrama e sem stop-words em textos lematizados.\n",
        "\n",
        "Extra:\n",
        "- 2.5 Contagem de termos simples (BoW) com unigrama, sem stop-words (combinando Spacy e NLTK) em textos com apenas verbos lematizados.\n",
        "\n",
        "Dica: crie uma função para lematizar o texto usando o Spacy, não esqueça de usar o POS-Tag quando necessário."
      ],
      "metadata": {
        "id": "6I_PEQasSIUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reposta\n",
        "\n",
        "# Carregue o dataset\n",
        "df = pd.read_csv(\"https://dados-ml-pln.s3-sa-east-1.amazonaws.com/produtos.csv\", delimiter=\";\", encoding='utf-8')\n",
        "\n",
        "# Limpeza e pré-processamento\n",
        "df.dropna(inplace=True)\n",
        "df[\"texto\"] = df['nome'] + \" \" + df['descricao']\n",
        "\n",
        "# Função para lematizar o texto (geral e apenas verbos)\n",
        "nlp = spacy.load(\"pt_core_news_sm\")\n",
        "def lematizar_texto(texto, apenas_verbos=False):\n",
        "    doc = nlp(texto)\n",
        "    if apenas_verbos:\n",
        "        tokens_lematizados = [token.lemma_ for token in doc if not token.is_stop and token.pos_ == 'VERB']\n",
        "    else:\n",
        "        tokens_lematizados = [token.lemma_ for token in doc if not token.is_stop]\n",
        "    return \" \".join(tokens_lematizados)\n",
        "\n",
        "df['texto_lematizado'] = df['texto'].apply(lematizar_texto)\n",
        "df['texto_verbos_lematizados'] = df['texto'].apply(lambda x: lematizar_texto(x, apenas_verbos=True))\n",
        "\n",
        "# Divisão da amostra entre treino e teste\n",
        "df_train, df_test = train_test_split(df, test_size=0.3, random_state=42)\n",
        "\n",
        "# Carregue as stop words do NLTK\n",
        "stop_words = nltk.corpus.stopwords.words('portuguese')\n",
        "\n",
        "# Configurações dos modelos\n",
        "configs = [\n",
        "    {'vectorizer': CountVectorizer(ngram_range=(1, 1)), 'text_col': 'texto'},  # 2.1\n",
        "    {'vectorizer': CountVectorizer(stop_words=stop_words, ngram_range=(1, 2)), 'text_col': 'texto'},  # 2.2\n",
        "    {'vectorizer': TfidfVectorizer(stop_words=stop_words, ngram_range=(1, 1)), 'text_col': 'texto'},  # 2.3\n",
        "    {'vectorizer': TfidfVectorizer(ngram_range=(1, 1)), 'text_col': 'texto_lematizado'},  # 2.4\n",
        "    {'vectorizer': CountVectorizer(stop_words=stop_words, ngram_range=(1, 1)), 'text_col': 'texto_verbos_lematizados'}  # 2.5\n",
        "]\n",
        "\n",
        "# Treinamento e avaliação dos modelos\n",
        "for i, config in enumerate(configs):\n",
        "    print(f\"Treinando modelo {i + 1}...\")\n",
        "\n",
        "    vectorizer = config['vectorizer']\n",
        "    text_col = config['text_col']\n",
        "\n",
        "    # Vetorização do dataframe de treino\n",
        "    vectorizer.fit(df_train[text_col])\n",
        "    x_train = vectorizer.transform(df_train[text_col])\n",
        "    y_train = df_train[\"categoria\"]\n",
        "\n",
        "    # Treinamento do modelo LogisticRegression\n",
        "    model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "    model.fit(x_train, y_train)\n",
        "\n",
        "    # Transforma o dataframe de teste em vetor (features)\n",
        "    x_test = vectorizer.transform(df_test[text_col])\n",
        "\n",
        "    # Escoragem da classificação na amostra de teste\n",
        "    y_prediction = model.predict(x_test)\n",
        "\n",
        "    # Mensuração do resultado pela acurácia\n",
        "    y_test = df_test[\"categoria\"]\n",
        "    accuracy = accuracy_score(y_prediction, y_test)\n",
        "    print(f\"Acurácia do modelo {i + 1}: {accuracy}\")"
      ],
      "metadata": {
        "id": "SbWyWKz8EDaS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}